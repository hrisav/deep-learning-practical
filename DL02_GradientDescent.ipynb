{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL02_GradientDescent.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3gvqxkMQdxq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn       # importing neural net module\n",
        "import numpy as np "
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Xm5WaYn9Cn"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4_Ou-3YYcBD"
      },
      "source": [
        "## Using Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx0-f-p7Y1MA"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Manually\n",
        "\n",
        "Loss Computation = Manually\n",
        "\n",
        "Parameter Updates = Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUmrGPReQjqt"
      },
      "source": [
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([4,5,6,7], dtype=np.float32)\n",
        "\n",
        "w = 0.0"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Nbrn01QIo4"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X\n",
        "\n",
        "\n",
        "# Loss: MSE = 1/N * (w*x - y)**2\n",
        "\n",
        "def loss(Yhat, Y):\n",
        "  return ((Yhat-Y)**2).mean()\n",
        "\n",
        "\n",
        "# dLoss/dw = 1/N * 2x(w*x - y)\n",
        "\n",
        "def gradient(Yhat, Y, X):\n",
        "  return (2*X*(Yhat-Y)).mean()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkDz252uyG9e",
        "outputId": "9d33b20a-4a81-48a5-bb5a-9d2c18787027"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)               # predict\n",
        "    mse = loss(Yhat, Y)             # loss\n",
        "    dw = gradient(Yhat, Y, X)       # gradient\n",
        "    w -= (learning_rate*dw)         # weight update\n",
        "\n",
        "    if (epoch%5==0):\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {mse:.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  0.0\n",
            "epoch 1: w = 0.030, loss = 31.50000\n",
            "epoch 6: w = 0.173, loss = 27.29192\n",
            "epoch 11: w = 0.306, loss = 23.67409\n",
            "epoch 16: w = 0.430, loss = 20.56374\n",
            "epoch 21: w = 0.544, loss = 17.88968\n",
            "epoch 26: w = 0.650, loss = 15.59071\n",
            "epoch 31: w = 0.748, loss = 13.61421\n",
            "epoch 36: w = 0.839, loss = 11.91496\n",
            "epoch 41: w = 0.924, loss = 10.45405\n",
            "epoch 46: w = 1.002, loss = 9.19807\n",
            "Output after training:  10.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWSDf2EQnhEY"
      },
      "source": [
        "## Using Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL3osOrWnnnO"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Autograd\n",
        "\n",
        "Loss Computation = Manually\n",
        "\n",
        "Parameter Updates = Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbPik6xYyHAl"
      },
      "source": [
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "Y = torch.tensor([4,5,6,7], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKrnYy5qOu_"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X\n",
        "\n",
        "# Loss: MSE = 1/N * (w*x - y)**2\n",
        "\n",
        "def loss(Yhat, Y):\n",
        "  return ((Yhat-Y)**2).mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OibgVSqBqOyA",
        "outputId": "bf40342e-31a6-4527-bf4a-1cf3eba3e377"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 200\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)                   # predict\n",
        "    mse = loss(Yhat, Y)                 # loss\n",
        "    mse.backward()                      # gradient\n",
        "\n",
        "    with torch.no_grad():               # deactivates autograd engine for fast computations\n",
        "        w -= (learning_rate*w.grad)     # weight update\n",
        "    \n",
        "    w.grad.zero_()                      # zero the gradients after updating weight\n",
        "\n",
        "    if (epoch%10==0):\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {mse.item():.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10).item(),2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  tensor([0.], grad_fn=<MulBackward0>)\n",
            "epoch 1: w = 0.030, loss = 31.50000\n",
            "epoch 11: w = 0.306, loss = 23.67409\n",
            "epoch 21: w = 0.544, loss = 17.88968\n",
            "epoch 31: w = 0.748, loss = 13.61421\n",
            "epoch 41: w = 0.924, loss = 10.45405\n",
            "epoch 51: w = 1.075, loss = 8.11827\n",
            "epoch 61: w = 1.205, loss = 6.39180\n",
            "epoch 71: w = 1.316, loss = 5.11571\n",
            "epoch 81: w = 1.412, loss = 4.17250\n",
            "epoch 91: w = 1.494, loss = 3.47534\n",
            "epoch 101: w = 1.565, loss = 2.96005\n",
            "epoch 111: w = 1.626, loss = 2.57918\n",
            "epoch 121: w = 1.679, loss = 2.29766\n",
            "epoch 131: w = 1.724, loss = 2.08958\n",
            "epoch 141: w = 1.763, loss = 1.93578\n",
            "epoch 151: w = 1.796, loss = 1.82210\n",
            "epoch 161: w = 1.825, loss = 1.73808\n",
            "epoch 171: w = 1.849, loss = 1.67597\n",
            "epoch 181: w = 1.870, loss = 1.63007\n",
            "epoch 191: w = 1.888, loss = 1.59614\n",
            "Output after training:  19.03\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i32ITTUCvxlT"
      },
      "source": [
        "## Using Loss & Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ9Hqma6v3td"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Autograd\n",
        "\n",
        "Loss Computation = PyTorch Loss\n",
        "\n",
        "Parameter Updates = PyTorch Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN73nNg5wNdB"
      },
      "source": [
        "1) Design model (input, output, forward pass with different layers)\n",
        "\n",
        "2) Construct loss and optimizer\n",
        "\n",
        "3) Training loop\n",
        "\n",
        "      Forward = compute prediction and loss\n",
        "\n",
        "      Backward = compute gradients\n",
        "\n",
        "      Update weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC3goN7FsA6d"
      },
      "source": [
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9i4yvVZwtJe"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4rH4jUYw2AX",
        "outputId": "3342cf08-f790-41b3-a6af-e0ee692ca1ef"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 200\n",
        "\n",
        "loss = nn.MSELoss()                 # for Loss import nn\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)               # predict\n",
        "    mse = loss(Yhat, Y)             # loss\n",
        "    mse.backward()                  # gradient\n",
        "\n",
        "    optimizer.step()                # update weights\n",
        "    \n",
        "    optimizer.zero_grad()          # zero the gradients after updating weight\n",
        "\n",
        "    if (epoch%10==0):\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {mse.item():.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10).item(),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  tensor(0.3000, grad_fn=<MulBackward0>)\n",
            "epoch 1: w = 0.090, loss = 29.10675\n",
            "epoch 11: w = 0.358, loss = 20.85378\n",
            "epoch 21: w = 0.588, loss = 15.41379\n",
            "epoch 31: w = 0.786, loss = 11.39289\n",
            "epoch 41: w = 0.956, loss = 8.42090\n",
            "epoch 51: w = 1.103, loss = 6.22420\n",
            "epoch 61: w = 1.229, loss = 4.60053\n",
            "epoch 71: w = 1.337, loss = 3.40042\n",
            "epoch 81: w = 1.430, loss = 2.51338\n",
            "epoch 91: w = 1.510, loss = 1.85773\n",
            "epoch 101: w = 1.579, loss = 1.37311\n",
            "epoch 111: w = 1.638, loss = 1.01492\n",
            "epoch 121: w = 1.688, loss = 0.75016\n",
            "epoch 131: w = 1.732, loss = 0.55447\n",
            "epoch 141: w = 1.770, loss = 0.40983\n",
            "epoch 151: w = 1.802, loss = 0.30292\n",
            "epoch 161: w = 1.830, loss = 0.22390\n",
            "epoch 171: w = 1.854, loss = 0.16549\n",
            "epoch 181: w = 1.874, loss = 0.12232\n",
            "epoch 191: w = 1.892, loss = 0.09041\n",
            "Output after training:  19.06\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o2OLQp7iylEa"
      },
      "source": [
        "## Using Model Loss & Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mrOWmJiTzWxT"
      },
      "source": [
        "Prediction = PyTorch Model\n",
        "\n",
        "Gradients Computation = Autograd\n",
        "\n",
        "Loss Computation = PyTorch Loss\n",
        "\n",
        "Parameter Updates = PyTorch Optimizer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eimv92mUyuKs"
      },
      "source": [
        "# training samples\n",
        "X = torch.tensor([[1], [2], [3], [4]], dtype=torch.float32)\n",
        "Y = torch.tensor([[2], [4], [6], [8]], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tOCq7AV3z1eL",
        "outputId": "0abae757-037b-4d41-91dc-899c723949d7"
      },
      "source": [
        "n_samples, n_features = X.shape\n",
        "n_samples, n_features"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4, 1)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JV7jiSXv0eSL"
      },
      "source": [
        "# test sample\n",
        "X_test = torch.tensor([5], dtype=torch.float32)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu_qxtTV1_Wk"
      },
      "source": [
        "input_size = n_features\n",
        "output_size = n_features\n",
        "\n",
        "model = nn.Linear(input_size, output_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bPFZEyPO2Dx_",
        "outputId": "a6884c01-656a-44be-fe49-3df8bd58ff7f"
      },
      "source": [
        "print('Output before training: ', model(X_test).item())\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 200\n",
        "\n",
        "loss = nn.MSELoss()                         # for Loss, import nn\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = model(X)                         # predict\n",
        "    mse = loss(Yhat, Y)                     # loss\n",
        "    mse.backward()                          # gradient\n",
        "\n",
        "    optimizer.step()                        # update weights\n",
        "    \n",
        "    optimizer.zero_grad()                   # zero the gradients after updating weight\n",
        "\n",
        "    if (epoch%10==0):\n",
        "        [w, b] = model.parameters()         # unpack parameters\n",
        "        print(f'epoch {epoch+1}: w = {w[0][0].item():.3f}, loss = {mse:.5f}')\n",
        "\n",
        "print('Output after training: ', round(model(X_test).item(),2))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  9.449021339416504\n",
            "epoch 1: w = 1.733, loss = 0.10309\n",
            "epoch 11: w = 1.734, loss = 0.10248\n",
            "epoch 21: w = 1.734, loss = 0.10187\n",
            "epoch 31: w = 1.735, loss = 0.10126\n",
            "epoch 41: w = 1.736, loss = 0.10065\n",
            "epoch 51: w = 1.737, loss = 0.10005\n",
            "epoch 61: w = 1.738, loss = 0.09946\n",
            "epoch 71: w = 1.738, loss = 0.09886\n",
            "epoch 81: w = 1.739, loss = 0.09827\n",
            "epoch 91: w = 1.740, loss = 0.09768\n",
            "epoch 101: w = 1.741, loss = 0.09710\n",
            "epoch 111: w = 1.741, loss = 0.09652\n",
            "epoch 121: w = 1.742, loss = 0.09594\n",
            "epoch 131: w = 1.743, loss = 0.09537\n",
            "epoch 141: w = 1.744, loss = 0.09480\n",
            "epoch 151: w = 1.745, loss = 0.09424\n",
            "epoch 161: w = 1.745, loss = 0.09367\n",
            "epoch 171: w = 1.746, loss = 0.09311\n",
            "epoch 181: w = 1.747, loss = 0.09256\n",
            "epoch 191: w = 1.748, loss = 0.09201\n",
            "Output after training:  9.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EwwJ9QKE5jhl"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}