{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DL02_GradientDescent.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3gvqxkMQdxq"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn       # importing neural net module\n",
        "import numpy as np "
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t4Xm5WaYn9Cn"
      },
      "source": [
        "# Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4_Ou-3YYcBD"
      },
      "source": [
        "## Using Numpy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vx0-f-p7Y1MA"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Manually\n",
        "\n",
        "Loss Computation = Manually\n",
        "\n",
        "Parameter Updates = Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JUmrGPReQjqt"
      },
      "source": [
        "X = np.array([1,2,3,4], dtype=np.float32)\n",
        "Y = np.array([4,5,6,7], dtype=np.float32)\n",
        "\n",
        "w = 0.0"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C4Nbrn01QIo4"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X\n",
        "\n",
        "\n",
        "# Loss: MSE = 1/N * (w*x - y)**2\n",
        "\n",
        "def loss(Yhat, Y):\n",
        "  return ((Yhat-Y)**2).mean()\n",
        "\n",
        "\n",
        "# dLoss/dw = 1/N * 2x(w*x - y)\n",
        "\n",
        "def gradient(Yhat, Y, X):\n",
        "  return (2*X*(Yhat-Y)).mean()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkDz252uyG9e",
        "outputId": "9d33b20a-4a81-48a5-bb5a-9d2c18787027"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 50\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)               # predict\n",
        "    mse = loss(Yhat, Y)             # loss\n",
        "    dw = gradient(Yhat, Y, X)       # gradient\n",
        "    w -= (learning_rate*dw)         # weight update\n",
        "\n",
        "    if (epoch%5==0):\n",
        "        print(f'epoch {epoch+1}: w = {w:.3f}, loss = {mse:.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10),2))"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  0.0\n",
            "epoch 1: w = 0.030, loss = 31.50000\n",
            "epoch 6: w = 0.173, loss = 27.29192\n",
            "epoch 11: w = 0.306, loss = 23.67409\n",
            "epoch 16: w = 0.430, loss = 20.56374\n",
            "epoch 21: w = 0.544, loss = 17.88968\n",
            "epoch 26: w = 0.650, loss = 15.59071\n",
            "epoch 31: w = 0.748, loss = 13.61421\n",
            "epoch 36: w = 0.839, loss = 11.91496\n",
            "epoch 41: w = 0.924, loss = 10.45405\n",
            "epoch 46: w = 1.002, loss = 9.19807\n",
            "Output after training:  10.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWSDf2EQnhEY"
      },
      "source": [
        "## Using Autograd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qL3osOrWnnnO"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Autograd\n",
        "\n",
        "Loss Computation = Manually\n",
        "\n",
        "Parameter Updates = Manually"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wbPik6xYyHAl"
      },
      "source": [
        "X = torch.tensor([1,2,3,4], dtype=torch.float32)\n",
        "Y = torch.tensor([4,5,6,7], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor([0.0], dtype=torch.float32, requires_grad=True)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzKrnYy5qOu_"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X\n",
        "\n",
        "# Loss: MSE = 1/N * (w*x - y)**2\n",
        "\n",
        "def loss(Yhat, Y):\n",
        "  return ((Yhat-Y)**2).mean()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OibgVSqBqOyA",
        "outputId": "ce799b9d-2eed-4855-bea0-a1a011a060dd"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 200\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)               # predict\n",
        "    mse = loss(Yhat, Y)             # loss\n",
        "    mse.backward()                  # gradient\n",
        "\n",
        "    with torch.no_grad():           # deactivates autograd engine for fast computations\n",
        "        w -= (learning_rate*dw)     # weight update\n",
        "    \n",
        "    w.grad.zero_()                  # zero the gradients after updating weight\n",
        "\n",
        "    if (epoch%10==0):\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {mse.item():.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10).item(),2))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  tensor([0.], grad_fn=<MulBackward0>)\n",
            "epoch 1: w = 0.014, loss = 31.50000\n",
            "epoch 11: w = 0.157, loss = 27.36189\n",
            "epoch 21: w = 0.300, loss = 23.53075\n",
            "epoch 31: w = 0.443, loss = 20.00657\n",
            "epoch 41: w = 0.587, loss = 16.78934\n",
            "epoch 51: w = 0.730, loss = 13.87909\n",
            "epoch 61: w = 0.873, loss = 11.27579\n",
            "epoch 71: w = 1.016, loss = 8.97946\n",
            "epoch 81: w = 1.159, loss = 6.99009\n",
            "epoch 91: w = 1.302, loss = 5.30769\n",
            "epoch 101: w = 1.445, loss = 3.93224\n",
            "epoch 111: w = 1.588, loss = 2.86376\n",
            "epoch 121: w = 1.731, loss = 2.10223\n",
            "epoch 131: w = 1.874, loss = 1.64766\n",
            "epoch 141: w = 2.017, loss = 1.50006\n",
            "epoch 151: w = 2.160, loss = 1.65941\n",
            "epoch 161: w = 2.303, loss = 2.12573\n",
            "epoch 171: w = 2.446, loss = 2.89902\n",
            "epoch 181: w = 2.589, loss = 3.97926\n",
            "epoch 191: w = 2.732, loss = 5.36648\n",
            "Output after training:  28.61\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i32ITTUCvxlT"
      },
      "source": [
        "## Using Loss & Optimizer functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQ9Hqma6v3td"
      },
      "source": [
        "Prediction = Manually\n",
        "\n",
        "Gradients Computation = Autograd\n",
        "\n",
        "Loss Computation = PyTorch Loss\n",
        "\n",
        "Parameter Updates = PyTorch Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iN73nNg5wNdB"
      },
      "source": [
        "1) Design model (input, output, forward pass with different layers)\n",
        "\n",
        "2) Construct loss and optimizer\n",
        "\n",
        "3) Training loop\n",
        "\n",
        "      Forward = compute prediction and loss\n",
        "\n",
        "      Backward = compute gradients\n",
        "\n",
        "      Update weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jC3goN7FsA6d"
      },
      "source": [
        "X = torch.tensor([1, 2, 3, 4], dtype=torch.float32)\n",
        "Y = torch.tensor([2, 4, 6, 8], dtype=torch.float32)\n",
        "\n",
        "w = torch.tensor(0.0, dtype=torch.float32, requires_grad=True)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R9i4yvVZwtJe"
      },
      "source": [
        "def forward(X):\n",
        "  return w*X"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B4rH4jUYw2AX",
        "outputId": "3342cf08-f790-41b3-a6af-e0ee692ca1ef"
      },
      "source": [
        "print('Output before training: ', forward(10))\n",
        "\n",
        "# Training\n",
        "learning_rate = 0.001\n",
        "n_iters = 200\n",
        "\n",
        "loss = nn.MSELoss()                 # for Loss\n",
        "optimizer = torch.optim.SGD([w], lr=learning_rate)\n",
        "\n",
        "for epoch in range(n_iters):\n",
        "\n",
        "    Yhat = forward(X)               # predict\n",
        "    mse = loss(Yhat, Y)             # loss\n",
        "    mse.backward()                  # gradient\n",
        "\n",
        "    optimizer.step()                # update weights\n",
        "    \n",
        "    optimizer.zero_grad()          # zero the gradients after updating weight\n",
        "\n",
        "    if (epoch%10==0):\n",
        "        print(f'epoch {epoch+1}: w = {w.item():.3f}, loss = {mse.item():.5f}')\n",
        "\n",
        "print('Output after training: ', round(forward(10).item(),2))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Output before training:  tensor(0.3000, grad_fn=<MulBackward0>)\n",
            "epoch 1: w = 0.090, loss = 29.10675\n",
            "epoch 11: w = 0.358, loss = 20.85378\n",
            "epoch 21: w = 0.588, loss = 15.41379\n",
            "epoch 31: w = 0.786, loss = 11.39289\n",
            "epoch 41: w = 0.956, loss = 8.42090\n",
            "epoch 51: w = 1.103, loss = 6.22420\n",
            "epoch 61: w = 1.229, loss = 4.60053\n",
            "epoch 71: w = 1.337, loss = 3.40042\n",
            "epoch 81: w = 1.430, loss = 2.51338\n",
            "epoch 91: w = 1.510, loss = 1.85773\n",
            "epoch 101: w = 1.579, loss = 1.37311\n",
            "epoch 111: w = 1.638, loss = 1.01492\n",
            "epoch 121: w = 1.688, loss = 0.75016\n",
            "epoch 131: w = 1.732, loss = 0.55447\n",
            "epoch 141: w = 1.770, loss = 0.40983\n",
            "epoch 151: w = 1.802, loss = 0.30292\n",
            "epoch 161: w = 1.830, loss = 0.22390\n",
            "epoch 171: w = 1.854, loss = 0.16549\n",
            "epoch 181: w = 1.874, loss = 0.12232\n",
            "epoch 191: w = 1.892, loss = 0.09041\n",
            "Output after training:  19.06\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}